{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kieren/SMU_MITB/CS611/cs611-assignment-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from utils.data_preprocessing_bronze_table import *\n",
    "from utils.data_preprocessing_silver_table import *\n",
    "from utils.helper import *\n",
    "\n",
    "\n",
    "current_directory = os.path.dirname(os.path.abspath(os.path.join(os.getcwd(), \"cs611-assignment-1\")))\n",
    "csv_dir = os.path.join(current_directory, \"data\")\n",
    "print(current_directory)\n",
    "\n",
    "# Other parts\n",
    "data_mart_dir = os.path.join(current_directory, \"datamart\")\n",
    "bronze_dir = os.path.join(data_mart_dir, \"bronze\")\n",
    "silver_dir = os.path.join(data_mart_dir, \"silver\")\n",
    "gold_dir = os.path.join(data_mart_dir, \"gold\")\n",
    "\n",
    "# Refresh current directory\n",
    "if os.path.exists(data_mart_dir):\n",
    "    shutil.rmtree(data_mart_dir)\n",
    "\n",
    "os.mkdir(data_mart_dir)\n",
    "os.mkdir(bronze_dir)\n",
    "os.mkdir(silver_dir)\n",
    "os.mkdir(gold_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_bronze(start_date, end_date, spark : SparkSession):\n",
    "    print('\\n\\n---starting Bronze Table job---\\n\\n')\n",
    "\n",
    "    # Get all the datetimes \n",
    "    dates_str_list = generate_first_of_month_dates(start_date, end_date)\n",
    "\n",
    "    # We can build the bronze table\n",
    "    # Get csvs\n",
    "    csv_files = os.listdir(csv_dir)\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        csv_full_dir = os.path.join(csv_dir, csv_file)\n",
    "        for date_str in dates_str_list:\n",
    "            print(\"Preparing bronze table {}\".format(csv_file))\n",
    "            prepare_bronze_table_daily(csv_full_dir, bronze_dir, spark, date_str)\n",
    "\n",
    "    # Now we can prepare the silver\n",
    "    # for csv_file in csv_files:\n",
    "    #     csv_full_dir = os.path.join(csv_dir, csv_file)\n",
    "    #     csv_type = csv_file.rstrip(\".csv\")\n",
    "\n",
    "    #     if csv_type == \"lms_loan_daily\":\n",
    "    #         # Get all the lms_loan_daily files\n",
    "    #         for date_str in dates_str_list:\n",
    "    #             expected_lms_loan_daily_file_name = \"bronze_\" + csv_type + \"_\" + date_str + \".csv\"\n",
    "    #             expected_full_dir = os.path.join(bronze_dir, expected_lms_loan_daily_file_name)\n",
    "\n",
    "    #             process_silver_table_loan_daily(expected_full_dir,\n",
    "    #                                             silver_dir,\n",
    "    #                                             date_str,\n",
    "    #                                             spark)\n",
    "                \n",
    "    #     elif csv_type == \"feature_finanicals\":\n",
    "    #         # Get all feature_financials files\n",
    "    #         for date_str in dates_str_list:\n",
    "    #             expected_feature_financials_file_name = \"bronze_\" + csv_type + \"_\" + date_str + \".csv\"\n",
    "    #             expected_full_dir = os.path.join(bronze_dir, expected_feature_financials_file_name)\n",
    "\n",
    "    #             process_silver_table_feature_financials(expected_full_dir,\n",
    "    #                                                     silver_dir,\n",
    "    #                                                     date_str,\n",
    "    #                                                     spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_silver(start_date, end_date, spark : SparkSession):\n",
    "    print('\\n\\n---starting Silver table job---\\n\\n')\n",
    "\n",
    "    # Get all the datetimes \n",
    "    dates_str_list = generate_first_of_month_dates(start_date, end_date)\n",
    "\n",
    "    # We can build the silver table\n",
    "    for date_str in dates_str_list:\n",
    "        # Build the silver table for each csv\n",
    "        expected_lms_loan_daily_file_name = \"bronze_lms_loan_daily_\" + date_str + \".csv\"\n",
    "        expected_loan_full_dir = os.path.join(bronze_dir, expected_lms_loan_daily_file_name)\n",
    "\n",
    "        process_silver_table_loan_daily(expected_loan_full_dir,\n",
    "                                        silver_dir,\n",
    "                                        date_str,\n",
    "                                        spark)\n",
    "        \n",
    "        expected_feature_financials_file_name = \"bronze_features_financial_\" + date_str + \".csv\"\n",
    "        expected_financial_full_dir = os.path.join(bronze_dir, expected_feature_financials_file_name)\n",
    "\n",
    "        process_silver_table_feature_financials(expected_financial_full_dir,\n",
    "                                                silver_dir,\n",
    "                                                date_str,\n",
    "                                                spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "        .appName(\"dev\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---starting Bronze Table job---\n",
      "\n",
      "\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-01-01 00:00:00 : 530\n",
      "Bronze features_financial Daily Date 2023-01-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-01-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-02-01 00:00:00 : 501\n",
      "Bronze features_financial Daily Date 2023-02-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-02-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-03-01 00:00:00 : 506\n",
      "Bronze features_financial Daily Date 2023-03-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-03-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-04-01 00:00:00 : 510\n",
      "Bronze features_financial Daily Date 2023-04-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-04-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-05-01 00:00:00 : 521\n",
      "Bronze features_financial Daily Date 2023-05-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-05-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-06-01 00:00:00 : 517\n",
      "Bronze features_financial Daily Date 2023-06-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-06-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-07-01 00:00:00 : 471\n",
      "Bronze features_financial Daily Date 2023-07-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-07-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-08-01 00:00:00 : 481\n",
      "Bronze features_financial Daily Date 2023-08-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-08-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-09-01 00:00:00 : 454\n",
      "Bronze features_financial Daily Date 2023-09-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-09-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-10-01 00:00:00 : 487\n",
      "Bronze features_financial Daily Date 2023-10-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-10-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-11-01 00:00:00 : 491\n",
      "Bronze features_financial Daily Date 2023-11-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-11-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2023-12-01 00:00:00 : 489\n",
      "Bronze features_financial Daily Date 2023-12-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-12-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-01-01 00:00:00 : 485\n",
      "Bronze features_financial Daily Date 2024-01-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-01-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-02-01 00:00:00 : 518\n",
      "Bronze features_financial Daily Date 2024-02-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-02-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-03-01 00:00:00 : 511\n",
      "Bronze features_financial Daily Date 2024-03-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-03-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-04-01 00:00:00 : 513\n",
      "Bronze features_financial Daily Date 2024-04-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-04-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-05-01 00:00:00 : 491\n",
      "Bronze features_financial Daily Date 2024-05-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-05-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-06-01 00:00:00 : 498\n",
      "Bronze features_financial Daily Date 2024-06-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-06-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-07-01 00:00:00 : 505\n",
      "Bronze features_financial Daily Date 2024-07-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-07-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-08-01 00:00:00 : 543\n",
      "Bronze features_financial Daily Date 2024-08-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-08-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-09-01 00:00:00 : 493\n",
      "Bronze features_financial Daily Date 2024-09-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-09-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-10-01 00:00:00 : 456\n",
      "Bronze features_financial Daily Date 2024-10-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-10-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-11-01 00:00:00 : 488\n",
      "Bronze features_financial Daily Date 2024-11-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-11-01.csv\n",
      "Preparing bronze table features_financials.csv\n",
      "Row Count for Date 2024-12-01 00:00:00 : 515\n",
      "Bronze features_financial Daily Date 2024-12-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2024-12-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-01-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-01-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-01-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-02-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-02-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-02-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-03-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-03-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-03-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-04-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-04-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-04-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-05-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-05-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-05-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-06-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-06-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-06-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-07-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-07-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-07-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-08-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-08-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-08-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-09-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-09-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-09-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-10-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-10-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-10-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-11-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-11-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-11-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2023-12-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2023-12-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2023-12-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-01-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-01-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-01-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-02-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-02-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-02-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-03-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-03-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-03-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-04-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-04-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-04-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-05-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-05-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-05-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-06-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-06-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-06-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-07-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-07-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-07-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-08-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-08-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-08-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-09-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-09-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-09-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-10-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-10-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-10-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-11-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-11-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-11-01.csv\n",
      "Preparing bronze table feature_clickstream.csv\n",
      "Row Count for Date 2024-12-01 00:00:00 : 8974\n",
      "Bronze feature_clickstream Daily Date 2024-12-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_feature_clickstream_2024-12-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-01-01 00:00:00 : 530\n",
      "Bronze features_attribute Daily Date 2023-01-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-01-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-02-01 00:00:00 : 501\n",
      "Bronze features_attribute Daily Date 2023-02-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-02-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-03-01 00:00:00 : 506\n",
      "Bronze features_attribute Daily Date 2023-03-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-03-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-04-01 00:00:00 : 510\n",
      "Bronze features_attribute Daily Date 2023-04-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-04-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-05-01 00:00:00 : 521\n",
      "Bronze features_attribute Daily Date 2023-05-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-05-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-06-01 00:00:00 : 517\n",
      "Bronze features_attribute Daily Date 2023-06-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-06-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-07-01 00:00:00 : 471\n",
      "Bronze features_attribute Daily Date 2023-07-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-07-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-08-01 00:00:00 : 481\n",
      "Bronze features_attribute Daily Date 2023-08-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-08-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-09-01 00:00:00 : 454\n",
      "Bronze features_attribute Daily Date 2023-09-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-09-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-10-01 00:00:00 : 487\n",
      "Bronze features_attribute Daily Date 2023-10-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-10-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-11-01 00:00:00 : 491\n",
      "Bronze features_attribute Daily Date 2023-11-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-11-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2023-12-01 00:00:00 : 489\n",
      "Bronze features_attribute Daily Date 2023-12-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2023-12-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-01-01 00:00:00 : 485\n",
      "Bronze features_attribute Daily Date 2024-01-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-01-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-02-01 00:00:00 : 518\n",
      "Bronze features_attribute Daily Date 2024-02-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-02-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-03-01 00:00:00 : 511\n",
      "Bronze features_attribute Daily Date 2024-03-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-03-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-04-01 00:00:00 : 513\n",
      "Bronze features_attribute Daily Date 2024-04-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-04-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-05-01 00:00:00 : 491\n",
      "Bronze features_attribute Daily Date 2024-05-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-05-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-06-01 00:00:00 : 498\n",
      "Bronze features_attribute Daily Date 2024-06-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-06-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-07-01 00:00:00 : 505\n",
      "Bronze features_attribute Daily Date 2024-07-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-07-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-08-01 00:00:00 : 543\n",
      "Bronze features_attribute Daily Date 2024-08-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-08-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-09-01 00:00:00 : 493\n",
      "Bronze features_attribute Daily Date 2024-09-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-09-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-10-01 00:00:00 : 456\n",
      "Bronze features_attribute Daily Date 2024-10-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-10-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-11-01 00:00:00 : 488\n",
      "Bronze features_attribute Daily Date 2024-11-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-11-01.csv\n",
      "Preparing bronze table features_attributes.csv\n",
      "Row Count for Date 2024-12-01 00:00:00 : 515\n",
      "Bronze features_attribute Daily Date 2024-12-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_attribute_2024-12-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-01-01 00:00:00 : 530\n",
      "Bronze lms_loan_daily Daily Date 2023-01-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-01-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-02-01 00:00:00 : 1031\n",
      "Bronze lms_loan_daily Daily Date 2023-02-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-02-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-03-01 00:00:00 : 1537\n",
      "Bronze lms_loan_daily Daily Date 2023-03-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-03-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-04-01 00:00:00 : 2047\n",
      "Bronze lms_loan_daily Daily Date 2023-04-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-04-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-05-01 00:00:00 : 2568\n",
      "Bronze lms_loan_daily Daily Date 2023-05-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-05-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-06-01 00:00:00 : 3085\n",
      "Bronze lms_loan_daily Daily Date 2023-06-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-06-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-07-01 00:00:00 : 3556\n",
      "Bronze lms_loan_daily Daily Date 2023-07-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-07-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-08-01 00:00:00 : 4037\n",
      "Bronze lms_loan_daily Daily Date 2023-08-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-08-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-09-01 00:00:00 : 4491\n",
      "Bronze lms_loan_daily Daily Date 2023-09-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-09-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-10-01 00:00:00 : 4978\n",
      "Bronze lms_loan_daily Daily Date 2023-10-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-10-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-11-01 00:00:00 : 5469\n",
      "Bronze lms_loan_daily Daily Date 2023-11-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-11-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2023-12-01 00:00:00 : 5428\n",
      "Bronze lms_loan_daily Daily Date 2023-12-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-12-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-01-01 00:00:00 : 5412\n",
      "Bronze lms_loan_daily Daily Date 2024-01-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-01-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-02-01 00:00:00 : 5424\n",
      "Bronze lms_loan_daily Daily Date 2024-02-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-02-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-03-01 00:00:00 : 5425\n",
      "Bronze lms_loan_daily Daily Date 2024-03-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-03-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-04-01 00:00:00 : 5417\n",
      "Bronze lms_loan_daily Daily Date 2024-04-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-04-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-05-01 00:00:00 : 5391\n",
      "Bronze lms_loan_daily Daily Date 2024-05-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-05-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-06-01 00:00:00 : 5418\n",
      "Bronze lms_loan_daily Daily Date 2024-06-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-06-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-07-01 00:00:00 : 5442\n",
      "Bronze lms_loan_daily Daily Date 2024-07-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-07-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-08-01 00:00:00 : 5531\n",
      "Bronze lms_loan_daily Daily Date 2024-08-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-08-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-09-01 00:00:00 : 5537\n",
      "Bronze lms_loan_daily Daily Date 2024-09-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-09-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-10-01 00:00:00 : 5502\n",
      "Bronze lms_loan_daily Daily Date 2024-10-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-10-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-11-01 00:00:00 : 5501\n",
      "Bronze lms_loan_daily Daily Date 2024-11-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-11-01.csv\n",
      "Preparing bronze table lms_loan_daily.csv\n",
      "Row Count for Date 2024-12-01 00:00:00 : 5531\n",
      "Bronze lms_loan_daily Daily Date 2024-12-01 00:00:00 saved to : /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2024-12-01.csv\n"
     ]
    }
   ],
   "source": [
    "data_prep_bronze(\"2023-01-01\", \"2024-12-01\", spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---starting Silver table job---\n",
      "\n",
      "\n",
      "Loaded /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_lms_loan_daily_2023-01-01.csv, row count 530\n",
      "saved to: /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/silver/silver_lms_loan_daily_2023-01-01.csv\n",
      "Loaded /home/kieren/SMU_MITB/CS611/cs611-assignment-1/datamart/bronze/bronze_features_financial_2023-01-01.csv, row count 530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/15 14:42:54 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/kieren/pytorch_env/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kieren/pytorch_env/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/kieren/SMU_MITB/CS611/cs611-assignment-1/utils/data_preprocessing_silver_table.py\", line 33, in parse_float\n",
      "    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/re/__init__.py\", line 177, in search\n",
      "    return _compile(pattern, flags).search(string)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'float'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/15 14:42:54 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/kieren/SMU_MITB/CS611/cs611-assignment-1/utils/data_preprocessing_silver_table.py\", line 33, in parse_float\n",
      "    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/re/__init__.py\", line 177, in search\n",
      "    return _compile(pattern, flags).search(string)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'float'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/15 14:42:54 ERROR Executor: Exception in task 0.0 in stage 731.0 (TID 1041)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/kieren/SMU_MITB/CS611/cs611-assignment-1/utils/data_preprocessing_silver_table.py\", line 33, in parse_float\n",
      "    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/re/__init__.py\", line 177, in search\n",
      "    return _compile(pattern, flags).search(string)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: expected string or bytes-like object, got 'float'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/15 14:42:54 ERROR TaskSetManager: Task 0 in stage 731.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/kieren/SMU_MITB/CS611/cs611-assignment-1/utils/data_preprocessing_silver_table.py\", line 33, in parse_float\n    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/re/__init__.py\", line 177, in search\n    return _compile(pattern, flags).search(string)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: expected string or bytes-like object, got 'float'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_prep_silver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023-01-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2024-12-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mdata_prep_silver\u001b[0;34m(start_date, end_date, spark)\u001b[0m\n\u001b[1;32m     18\u001b[0m expected_feature_financials_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbronze_features_financial_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m date_str \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m expected_financial_full_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(bronze_dir, expected_feature_financials_file_name)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mprocess_silver_table_feature_financials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_financial_full_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msilver_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdate_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SMU_MITB/CS611/cs611-assignment-1/utils/data_preprocessing_silver_table.py:226\u001b[0m, in \u001b[0;36mprocess_silver_table_feature_financials\u001b[0;34m(bronze_feature_financials, silver_table_dir, date, spark)\u001b[0m\n\u001b[1;32m    224\u001b[0m df_exploded \u001b[38;5;241m=\u001b[39m df_split\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, explode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan_type_array\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    225\u001b[0m df_normalized \u001b[38;5;241m=\u001b[39m df_exploded\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, trim(lower(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m--> 226\u001b[0m loan_types \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloan_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloan_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loan \u001b[38;5;129;01min\u001b[39;00m loan_types:\n\u001b[1;32m    229\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m    230\u001b[0m         loan,\n\u001b[1;32m    231\u001b[0m         when(lower(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType_of_Loan\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcontains(loan), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    232\u001b[0m     )\n",
      "File \u001b[0;32m~/pytorch_env/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m~/pytorch_env/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/pytorch_env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/kieren/SMU_MITB/CS611/cs611-assignment-1/utils/data_preprocessing_silver_table.py\", line 33, in parse_float\n    match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/re/__init__.py\", line 177, in search\n    return _compile(pattern, flags).search(string)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: expected string or bytes-like object, got 'float'\n"
     ]
    }
   ],
   "source": [
    "data_prep_silver(\"2023-01-01\", \"2024-12-01\", spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
